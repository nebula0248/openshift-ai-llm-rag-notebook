{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a359bd-4f69-4e88-82c0-5763c26aa0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q einops==0.7.0 langchain==0.1.9 pymilvus==2.3.6 sentence-transformers==2.4.0 openai==1.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e11d23-c0ad-4875-b67f-149fc8b14725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3155f6ce-9deb-4d66-80da-4bf127901db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_milvus_vector_store(embedding_model_name, vector_db_collection_name):\n",
    "    # Create embedding\n",
    "    model_kwargs = {\n",
    "        'device': 'cuda',\n",
    "        'trust_remote_code': True,\n",
    "    }\n",
    "    encode_kwargs = {\n",
    "        'normalize_embeddings': False\n",
    "    }\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Connect to vector DB\n",
    "    return Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT, \"user\": MILVUS_USERNAME, \"password\": MILVUS_PASSWORD},\n",
    "        collection_name=vector_db_collection_name,\n",
    "        metadata_field=\"metadata\",\n",
    "        text_field=\"page_content\",\n",
    "        drop_old=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c1b1a",
   "metadata": {},
   "source": [
    "# Program starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f047be41-4da7-4c1f-89a7-4d4db53ed3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_SERVER_URL = \"https://qwen-25-7b-instruct-peter-lm.apps.rosa.peter-rosa-hcp.y0r0.p3.openshiftapps.com/v1\"\n",
    "MODEL_NAME = \"qwen-25-7b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef22a8ff-df62-47cd-98db-27c1e691b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=1024\n",
    "TOP_P=0.95\n",
    "TEMPERATURE=0.01\n",
    "PRESENCE_PENALTY=1.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad7c531-b994-4b14-ac0c-a1228db8d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "MILVUS_HOST = \"vectordb-milvus.milvus.svc.cluster.local\"\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_USERNAME = os.getenv('MILVUS_USERNAME')\n",
    "MILVUS_PASSWORD = os.getenv('MILVUS_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a3a2b",
   "metadata": {},
   "source": [
    "### Apply Prompt Engineering, define prompt instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2768eaef-41d1-4980-bfc2-dfc01dc8a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "    <s>[INST] <<SYS>>\n",
    "    You are a helpful, respectful and honest assistant named HatBot answering questions.\n",
    "    You will be given a question you need to answer, and a context to provide you with information. You must answer the question based as much as possible on this context.\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "    \n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "    <</SYS>>\n",
    "    \n",
    "    Context: \n",
    "    {context}\n",
    "    \n",
    "    Question: {question} [/INST]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584eb46b-144a-43bc-97d6-d4d0dd12412b",
   "metadata": {},
   "source": [
    "### Connect to the language model (vLLM) server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8fd396-0798-45c5-8969-6b6ede134c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "language_model = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=INFERENCE_SERVER_URL,\n",
    "    model_name=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fff5c-3ff7-4aa8-a054-2c40b24eedf9",
   "metadata": {},
   "source": [
    "### Integrate the vector DB and the vLLM server, so that whenever there is a query, it retrives from vector DB first, which implements RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04de6766-df85-4e56-a2e2-b699079ccdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = connect_milvus_vector_store(\n",
    "    embedding_model_name=\"ibm-granite/granite-embedding-278m-multilingual\",\n",
    "    vector_db_collection_name=\"openshift_ai_2_16_en_document\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    language_model,\n",
    "    retriever=vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 10}\n",
    "    ),\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": chatbot_prompt_template\n",
    "    },\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45ad23",
   "metadata": {},
   "source": [
    "### Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105d2fd1-f36c-409d-8e52-ec6d23a56ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfeed7b8035491d807099bdd00f777a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    To work with GPUs and taints in OpenShift AI, you need to follow these steps:\n",
      "\n",
      "1. **Enable GPU Support**:\n",
      "   - Ensure that the NVIDIA GPU Operator is installed on your OpenShift cluster.\n",
      "   - If you are in a disconnected or airgapped environment, follow the instructions provided by NVIDIA to deploy the GPU Operators.\n",
      "\n",
      "2. **Delete the `migration-gpu-status` ConfigMap**:\n",
      "   - Switch to the Administrator perspective in the OpenShift web console.\n",
      "   - Set the Project to All Projects or redhat-ods-applications.\n",
      "   - Search for the `migration-gpu-status` ConfigMap.\n",
      "   - Click the action menu (â‹®) and select Delete ConfigMap from the list.\n",
      "   - Confirm the deletion of the correct ConfigMap.\n",
      "\n",
      "3. **Apply Taints to Nodes or MachineSets**:\n",
      "   - Apply taints to your nodes or machine sets to restrict access to GPUs. For example:\n",
      "     ```yaml\n",
      "     apiVersion: machine.openshift.io/v1beta1\n",
      "     kind: MachineSet\n",
      "     metadata:\n",
      "       ...\n",
      "     spec:\n",
      "       replicas: 1\n",
      "       selector:\n",
      "         ...\n",
      "       template:\n",
      "         ...\n",
      "         spec:\n",
      "           ...\n",
      "           taints:\n",
      "             - key: restrictedaccess\n",
      "               value: \"yes\"\n",
      "               effect: NoSchedule\n",
      "     ```\n",
      "\n",
      "4. **Apply Tolerations to Pods**:\n",
      "   - Apply tolerations to the pods that require GPU resources. This ensures that the pods are scheduled on nodes with the appropriate taints.\n",
      "   - Example pod specification:\n",
      "     ```yaml\n",
      "     spec:\n",
      "       containers:\n",
      "       - name: app\n",
      "         image: ...\n",
      "         resources:\n",
      "           requests:\n",
      "             memory: \"64Mi\"\n",
      "             cpu: \"250m\"\n",
      "             nvidia.com/gpu: 2\n",
      "           limits:\n",
      "             memory: \"128Mi\"\n",
      "             cpu: \"500m\"\n",
      "         tolerations:\n",
      "         - key: \"restrictedaccess\"\n",
      "           operator: \"Equal\"\n",
      "           value: \"yes\"\n",
      "           effect: \"NoSchedule\"\n",
      "     ```\n",
      "\n",
      "5. **Configure NVIDIA GPU Operator Settings**:\n",
      "   - Navigate to the Installed Operator menu in the nvidia-gpu-operator namespace.\n",
      "   - Open the NVIDIA GPU Operator settings.\n",
      "   - Go to the ClusterPolicy tab and edit the ClusterPolicy as needed.\n",
      "\n",
      "By following these steps, you can effectively manage GPU resources and apply taints to control pod scheduling in your OpenShift AI environment. This ensures that only the necessary pods are scheduled on nodes with the required GPU resources, enhancing resource management and security. [/INST]"
     ]
    }
   ],
   "source": [
    "question = \"How can I work with GPU and taints in OpenShift AI?\"\n",
    "result = qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d75d0c",
   "metadata": {},
   "source": [
    "### Retrieve source of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acda357e-558a-4879-8ad8-21f0567f2f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ai-on-openshift.io/odh-rhoai/nvidia-gpus/\n",
      "https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/installing_and_uninstalling_openshift_ai_self-managed/index\n",
      "https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/introduction_to_red_hat_openshift_ai/index\n",
      "https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/release_notes/index\n",
      "https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html-single/installing_and_uninstalling_openshift_ai_self-managed_in_a_disconnected_environment/index\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in unique_list:\n",
    "            unique_list.append(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934b32c-46a6-4e82-8806-cc3e7e9939b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
